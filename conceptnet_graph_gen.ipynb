{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03062dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64218dcb",
   "metadata": {},
   "source": [
    "        phrase2 = token.split(\"â€™\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a497bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = np.loadtxt(\"tokenized_vocabs_no_utf.txt\", dtype='<U92',encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b628131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1669a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alpha(s):\n",
    "    return s.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc19b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ascii_vocab = []\n",
    "for vocab in vocabs:\n",
    "    if(is_ascii(vocab) is False or is_alpha(vocab) is False):\n",
    "        continue\n",
    "    real_ascii_vocab.append(vocab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c6f3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ascii_vocab_num = []\n",
    "for vocab in vocabs:\n",
    "    if(is_ascii(vocab) is False):\n",
    "        continue\n",
    "    real_ascii_vocab_num.append(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22af97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ascii_vocab.append(\"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "611e49e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "698114"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_ascii_vocab_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d99c466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695347"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_ascii_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73ceedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import node2vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3663bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f559778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qkrwo\\anaconda3\\envs\\node2vec\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(\"./conceptnet_modify_update.csv\", index_col=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb7d4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nodes(G, df, col, type_name):\n",
    "    \"\"\"Add entities to G from the 'col' column of the 'df' DataFrame. The new nodes are annotated with 'type_name' label.\"\"\"\n",
    "    nodes = list(df[~df[col].isnull()][col].unique())\n",
    "    G.add_nodes_from([(n,dict(type=type_name)) for n in nodes])\n",
    "    print(\"Nodes (%s,%s) were added\" % (col, type_name))\n",
    "    \n",
    "def add_nodes_2col(G, df, col1, col2, type_name):\n",
    "    \"\"\"Add entities to G from the 'col' column of the 'df' DataFrame. The new nodes are annotated with 'type_name' label.\"\"\"\n",
    "    nodes = list((df[~df[col1].isnull()][col1] + df[~df[col2].isnull()][col2]).unique())\n",
    "    print(len(nodes))\n",
    "    G.add_nodes_from([(n,dict(type=type_name)) for n in nodes])\n",
    "    print(\"Nodes (%s,%s) were added\" % (col1, type_name))\n",
    "    print(\"Nodes (%s,%s) were added\" % (col2, type_name))\n",
    "    print(word_nodes)\n",
    "    \n",
    "def add_links(G, df, col1, col2, type_name):\n",
    "    \"\"\"Add links to G from the 'df' DataFrame. The new edges are annotated with 'type_name' label.\"\"\"\n",
    "    df_tmp = df[(~df[col1].isnull()) & (~df[col2].isnull())]\n",
    "    links = list(zip(df_tmp[col1],df_tmp[col2]))\n",
    "    G.add_edges_from([(src, trg, dict(type=type_name)) for src, trg in links])\n",
    "    print(\"Edges (%s->%s,%s) were added\" % (col1, col2, type_name))\n",
    "    \n",
    "def add_relation(G, df, col1, col2, relation_name):\n",
    "    \"\"\"Add links to G from the 'df' DataFrame. The new edges are annotated with 'type_name' label.\"\"\"\n",
    "    df_tmp = df[~(df['relation'] != relation_name)]\n",
    "    links = list(zip(df_tmp[col1],df_tmp[col2]))\n",
    "    G.add_edges_from([(src, trg, dict(type=relation_name)) for src, trg in links])\n",
    "    print(\"Edges (%s->%s,%s) were added\" % (col1, col2, relation_name))\n",
    "\n",
    "    \n",
    "def add_relation_words(G, df, col1, col2, relation_name):\n",
    "    \"\"\"Add links to G from the 'df' DataFrame. The new edges are annotated with 'type_name' label.\"\"\"\n",
    "    df_tmp = df[~(df['relation'] != relation_name)]\n",
    "    links = list(zip(df_tmp[col1],df_tmp[col2]))\n",
    "    word_links = []\n",
    "    rel_links = []\n",
    "    for src, trg in links:\n",
    "        src, trg = str(src), str(trg)\n",
    "        src_words = src.split(\"_\")\n",
    "        trg_words = trg.split(\"_\")\n",
    "        for i in range(len(src_words) - 1) :\n",
    "            word_links.append((src_words[i], src_words[i+1], dict(type=\"sub_continous\")))\n",
    "        rel_links.append((src_words[-1], trg_words[0], dict(type=relation_name)))\n",
    "        for i in range(len(trg_words) - 1) :\n",
    "            word_links.append((trg_words[i], trg_words[i+1], dict(type=\"val_continous\")))\n",
    "    G.add_edges_from(word_links)\n",
    "    G.add_edges_from(rel_links)\n",
    "    print(\"relation number = \", len(rel_links))\n",
    "    print(\"Edges (%s->%s,%s) were added\" % (col1, col2, relation_name))\n",
    "    \n",
    "def add_relation_words2(G, df, col1, col2, relation_name):\n",
    "    \"\"\"Add links to G from the 'df' DataFrame. The new edges are annotated with 'type_name' label.\"\"\"\n",
    "    df_tmp = df[~(df['relation'] != relation_name)]\n",
    "    links = list(zip(df_tmp[col1],df_tmp[col2]))\n",
    "    word_links = []\n",
    "    rel_links = []\n",
    "    for src, trg in links:\n",
    "        src, trg = str(src), str(trg)\n",
    "        rel_links.append((src.split(\"/\")[0], trg.split(\"/\")[0], dict(type=relation_name)))\n",
    "    G.add_edges_from(rel_links)\n",
    "    print(\"Edges (%s->%s,%s) were added %s\" % (col1, col2, relation_name, len(rel_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5098a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202feb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
